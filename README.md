# ğŸ” Repository for "How Low Can You Go? The Data-Light SE Challenge"
---

## ğŸ“„ Summary

This repository contains the code, scripts, and data used to reproduce the results from our paper:

> **"How Low Can You Go? The   Data-Light SE Challenge"**  
> _Submitted to FSE 2026

We present the **BINGO effect**, a prevalent data compression phenomenon in software engineering (SE) optimization. Leveraging this, we show that **simple optimizers**â€”`RANDOM`, `LITE`, `LINE`â€”perform on par with the state-of-the-art optimizers, while running up to **10,000Ã— faster**.

---
## ğŸ§ª Experimental Setup

All experiments were run on a 4-core Linux (Ubuntu 24.04) system (1.30GHz, 16GB RAM, no GPU).

### Configuration

- **Datasets**: 39 MOOT tasks in `data/moot/`
- **Repeats**: 20 runs per optimizer
- **Budgets**: {6, 12, 18, 24, 50, 100, 200}
- **Optimizers**: `DEHB`, `SMAC`, `NSGAIII`, `TPE`, `LITE`, `LINE`, `RANDOM`
- **Evaluation**:
  - Effectiveness/ Benefit: distance-to-heaven (multi-objective)
  - Cost: no. of accessed labels, wall-clock time
---
## ğŸ“Š Reproducing the Results (Table 4, Figures 4 & 5)

These instructions reproduce all core results from the paper, including **Table 4**, **Figure 4**, and **Figure 5**.

All experiments were run using **Python 3.13**.

---

### â¤ Step 1: Install Dependencies

```bash
pip install -r requirements.txt
```

---

### â¤ Step 2: Generate Table 4

```bash
cd experiments/LUA_run_all/
make comparez
make report
```

The output will be saved to:

```
results/optimization_performance/report.csv
```

---

### â¤ Step 3: Generate Figure 4 (%Best vs. Label Budget)

```bash
cd experiments/
python3 optim_performance_comp.py
```

---

### â¤ Step 4: Generate Figure 5 (Runtime Comparison)

```bash
cd experiments/
python3 performance_box.py
```

---

### ğŸ§ª Optional: Re-run Optimizers

We include precomputed results for `DEHB`, `SMAC`, `NSGAIII`, `TPE`, and `LITE` to save time. To regenerate:

```bash
# A. Remove existing results
rm -rf results  #removes all results

# B. Generate commands
make generate-commands NAME=Active_Learning   # This is LITE or use NAME=DEHB or SMAC or TPE or NSGAIII

# C. Run the optimizer
cd experiments/
./commands.sh
```

---

## ğŸ“¦ Repository Structure

```plaintext
.
â”œâ”€â”€ data/                         # Input data directory with MOOT datasets: 127 SE optimization tasks

â”œâ”€â”€ active_learning/              # Active learning source code
â”‚   â”œâ”€â”€ LICENSE.md                # Original license (MIT)
â”‚   â””â”€â”€ src/
â”‚       â””â”€â”€ bl.py                 # Contains Bayesian active learner

â”œâ”€â”€ experiments/                  # Scripts for running experiments and generating plots/tables
â”‚   â”œâ”€â”€ FileResultsReader.py      # Reads optimizer result files
â”‚   â”œâ”€â”€ LUA_run_all/              # Lua scripts containing LITE and TABLE V generation logic
â”‚   â”‚   â”œâ”€â”€ Makefile              # Automates command/script generation
â”‚   â”‚   â”œâ”€â”€ run_all.lua           # Generates TABLE V
â”‚   â”‚   â””â”€â”€ stats.lua             # Scott-Knott/effect size stats logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ experiement_runner_parallel.py  # Runs the optimizers
â”‚   â”œâ”€â”€ optim_performance_comp.py       # Script to generate Fig. 5
â”‚   â””â”€â”€ performance_box.py           # Script to generate Fig. 6

â”œâ”€â”€ models/                       # Manages and evaluates configs
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ configurations/
â”‚   â”‚   â””â”€â”€ model_config_static.py   # Reads and manages tabular configs from MOOT
|   â””â”€â”€ Data.py     # Class for maintaining data, caching and KD-Tree
â”‚   â””â”€â”€ model_wrapper_static.py     # Wrapper class for config evaluation

â”œâ”€â”€ optimizers/                        # Optimizers implemented in the paper
â”‚   â”œâ”€â”€ ActLearnOptimizer.py           # Active learning optimizer (LITE)
â”‚   â”œâ”€â”€ DEHBOptimizer.py               # DEHB optimizer
â”‚   â”œâ”€â”€ NSGAIIIOptimizer.py            # Multi-objective evolutionary optimizer (NSGA-III)
â”‚   â”œâ”€â”€ SMACOptimizer.py               # SMAC (Sequential Model-Based Algorithm Configuration)
â”‚   â”œâ”€â”€ TPEOptimizer.py                # Tree-structured Parzen Estimator optimizer
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ base_optimizer.py              # Abstract base class for all optimizers

â”œâ”€â”€ results/                          # Output directories for optimizer runs
â”‚   â”œâ”€â”€ results_Active_Learning/      # Results from LITE
â”‚   â”œâ”€â”€ results_DEHB/                 # Results from DEHB
â”‚   â”œâ”€â”€ results_NSGAIII/              # Results from NSGA-III
â”‚   â”œâ”€â”€ results_SMAC/                 # Results from SMAC
â”‚   â””â”€â”€ results_TPE/                  # Results from TPE


â””â”€â”€ utils/                        # Utility scripts and shared functions
    â”œâ”€â”€ DistanceUtil.py           # Computes "distance to heaven"
    â”œâ”€â”€ LoggingUtil.py            # Sets up and manages logging
    â”œâ”€â”€ LoggingUtil.py            # Encodes/Decodes different data types
    â”œâ”€â”€ __init__.py
    â””â”€â”€ data_loader_templated.py  # Loads and parses CSV datasets

â”œâ”€â”€ .gitignore                    # Ignore logs, cache, and other non-reproducible files
â”œâ”€â”€ LICENSE                       # MIT license (temporarily redacted)
â”œâ”€â”€ Makefile                      # Automates command/script generation and execution
â”œâ”€â”€ README.md                     # Artifact overview and reproduction instructions
â””â”€â”€ requirements.txt              # Python dependencies for experiments and plotting
```

---

## âš™ï¸ Optimizers

| Optimizer | Description |
|----------|-------------|
| `RANDOM` | Random sampling of bucketed data |
| `LITE`   | Naive Bayes-based active learner (selects high g/r) |
| `LINE`   | Diversity sampling via KMeans++ |
| `DEHB`   | Differential Evolution + Hyperband |
| `NSGAIII`| Multi-objective evolutionary optimization |
| `SMAC`   | Model-based Bayesian optimization |
| `TPE`    | Parzen estimator-based Bayesian optimization |


---

## ğŸ” License

> Includes MIT-licensed components.

---

## ğŸ”— External Links

> Will be updated upon acceptance:
- ğŸ“œ Paper DOI
- ğŸ“ Dataset DOI
- ğŸ§ª Artifact DOI


---




